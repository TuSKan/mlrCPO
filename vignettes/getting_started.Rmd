---
title: "First Steps"
author: "Martin Binder"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{First Steps}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---


```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# Preprocessing Operators

Data preprocessing can be performed using the "mlrCPO" ("Composable Preprocessing Operators") addon package for mlr.
CPO makes it easy to use a variety of preprocessing operations, to chain different operations, and to integrate
preprocessing with mlr learners, and to define custom preprocessing operations.

To chain different operations, to apply a CPO to a dataset, and to attach a CPO to a learner, the operator `%>>%` can
be used. This way, it is possible to quickly create natural looking pipelines that are very flexible and can even be
optimized over.

The following requires the `mlrCPO` package to be loaded:
```{r}
library("mlrCPO")
```

## CPO Objects

Different preprocessing operations are provided in the form of *CPO Constructors*, which can be called like functions
to create *CPO* objects. These CPO objects are then used to apply the operation to a data set.
```{r}
cpoScale  # a cpo constructor
```
```{r}
print(cpoScale, verbose = TRUE)  # more information
cpoScale()  # create a CPO object that scales and centers data
```

`CPO` objects are central to `mlrCPO`, and they are very flexible. They can be applied to a dataset or a Task:
```{r}
head(iris %>>% cpoScale())
```
```{r}
head(getTaskData(iris.task %>>% cpoScale()))
```
they can be concatenated to create new operations:
```{r}
cpoScale() %>>% cpoPca()
```
and they can be fused with a `Learner` to create a machine learning pipeline that performs preprocessing on the learning data
and also pre-processes the data that is fed to the model for prediction.

A list of all internal `CPO`s can be retrieved using `listCPO`, which returns a data frame of names, descriptions, and categories.
```{r}
listCPO()$name
```

## Retrafo

Manipulating data for preprocessing itself is relatively easy. A challenge comes when one wants to integrate preprocessing
into a machine-learning pipeline: The same preprocessing steps that are performed on the training data need to be performed
on the new prediction data. However, the transformation performed for prediction often needs information from the training step.
For example, if training entail performing PCA, then for prediction, the data must not undergo another PCA, instead it needs
to be rotated by the rotation matrix found by the training PCA. The process of obtaining the rotation matrix will be called
"training" the `CPO`, and the object that contains the trained information is called `CPOTrained`. For preprocessing operations
that operate only on *features* of a task (as opposed to the target column), the `CPOTrained` will always be applied to new
incoming data, and hence be of class `CPORetrafo` and called a "retrafo" object. To obtain this retrafo object, one can use
`retrafo()`. Retrafo objects can be applied to data just as `CPO`s can, by using the `%>>%` operator.

```{r}
transformed = iris %>>% cpoPca()
ret = retrafo(transformed)
print(ret)
```
Comparing the first lines of the transformed data with the lines of "retransforming" the same input data:
```{r}
head(transformed)
head(iris) %>>% ret
```
Note that using the PCA on only the head of 'iris' gives a different (wrong) result:
```{r}
head(iris) %>>% cpoPca()
```

When attaching a `CPO` to a `Learner` using the `%>>%`-operator, all this is performed by `mlrCPO`, so there is no need to
worry about keeping `CPOTrained` objects around.

`retrafo()` gets the retrafo object from an attribute of the `transformed` object. The retrafo is always generated by the application
of a `CPO` already. Retrafos that are attached to an object also stack, so multiple applications of different `CPO`s only need
one `retrafo()` call to get the respective retrafo operation.
```{r}
trans.a = iris %>>% cpoScale()
trans.b = trans.a %>>% cpoPca()
ret = retrafo(trans.b)
```
Check that 'ret' does what it is supposed to do:
```{r}
head(trans.b)
head(iris) %>>% ret
```

## Hyperparameters

`CPO` objects have hyperparameters that can be adjusted at creation, or later using `setHyperPars`.
```{r}
print(cpoScale)
```
```{r}
do.center = cpoScale(scale = FALSE, center = TRUE)
print(do.center, verbose = TRUE)  # note the 'scale.' prefix of parameters
```
```{r}
do.scale = setHyperPars(do.center,
  scale.scale = TRUE, scale.center = FALSE)
```

These hyperparameters even survive `CPO` composition and attachment to Learners:
```{r}
cpo = cpoScale() %>>% cpoPca()
lrn = cpo %>>% makeLearner("classif.logreg")
print(lrn)
```

### Tuning
Since the hyperparameters remain intact for learners, is possible to tune hyperparameters parameters of preprocessing operations.

```{r}
(clrn = cpoModelMatrix() %>>% makeLearner("classif.logreg"))
```
```{r}
getParamSet(clrn)
```
```{r}
ps = makeParamSet(
    makeDiscreteParam(
        "model.matrix.formula",
        values = list(first = ~0 + ., second = ~0 + .^2, third = ~0 + .^3)))

tuneParams(clrn, pid.task, cv5, par.set = ps,
           control = makeTuneControlGrid(),
           show.info=TRUE)
```

## Special CPOs

### CPO Multiplexer
The multiplexer makes it possible to combine many CPOs into one, with an extra `selected.cpo` parameter that chooses between them.

```{r}
cpm = cpoMultiplex(list(cpoScale, cpoPca))
print(cpm, verbose = TRUE)
```
```{r}
head(iris %>>% setHyperPars(cpm, selected.cpo = "scale"))
```
Every CPO's Hyperparameters are exported:
```{r}
head(iris %>>% setHyperPars(cpm, selected.cpo = "scale", scale.center = FALSE))
```
```{r}
head(iris %>>% setHyperPars(cpm, selected.cpo = "pca"))
```

This makes it possible to tune over many different tuner configurations at once.

### CPO Wrapper
A simple CPO with one parameter which gets applied to the data as CPO. This is different from a multiplexer in that its parameter is free and can take any value that behaves like a CPO. On the downside, this does not expose the argument's parameters to the outside.

```{r}
cpa = cpoWrap()
print(cpa, verbose = TRUE)
```
```{r}
head(iris %>>% setHyperPars(cpa, wrap.cpo = cpoScale()))
```
```{r}
head(iris %>>% setHyperPars(cpa, wrap.cpo = cpoPca()))
```
Attaching the cpo applicator to a learner gives this learner a "cpo" hyperparameter that can be set to any CPO.
```{r}
getParamSet(cpoWrap() %>>% makeLearner("classif.logreg"))
```

### CBind CPO
`cbind` other CPOs as operation. The `cbinder` makes it possible to build DAGs of CPOs that perform different operations on data and paste the results next to each other.

```{r}
scale = cpoScale(id = "scale")
scale.pca = scale %>>% cpoPca()
cbinder = cpoCbind(scaled = scale, pcad = scale.pca, original = NULLCPO)
```
`cpoCbind` recognises that `"scale.scale"` happens before `"pca.pca"` but is also fed to the result directly. The summary draws a (crude) ascii-art graph.
```{r}
print(cbinder)
```
```{r}
getParamSet(cbinder)
```
```{r}
head(iris %>>% cbinder)
```
The unnecessary copies of "Species" are unfortunate. Remove them with cpoSelect:
```{r}
selector = cpoSelect(type = "numeric")
cbinder.select = cpoCbind(scaled = selector %>>% scale, pcad = selector %>>% scale.pca, original = NULLCPO)
cbinder.select
head(iris %>>% cbinder)
```
alternatively, we apply the cbinder only to numerical data
```{r}
head(iris %>>% cpoWrap(cbinder, affect.type = "numeric"))
```

## Custom CPOs

Even though `CPO`s are very flexible and can be combined in many ways, it may be necessary to create completely custom `CPO`s.
Custom CPOs can be created using the `makeCPO` function. Its most important arguments are `cpo.trafo` and `cpo.retrafo`, both of which are functions.
In principle, a `CPO` needs a function that "trains" a control object depending on the data (`cpo.trafo`),
and another function that uses this control object, and new data, to perform the preprocessing operation (`cpo.retrafo`).
The `cpo.trafo`-function must return a "control" object which contains all information about how to transform a given dataset.
`cpo.retrafo` takes a (potentially new!) dataset *and* the "control" object returned by `cpo.trafo`, and transforms the new data according to plan.
```{r}
names(formals(makeCPO))  # see help(makeCPO) for explanation of arguments
```

```{r}
constFeatRem = makeCPO("constFeatRem",
  dataformat = "df.features",
  cpo.train = function(data, target) {
    names(Filter(function(x) {  # names of columns to keep
        length(unique(x)) > 1
      }, data))
    }, cpo.retrafo = function(data, control) {
    data[control]
  })
head(iris) %>>% constFeatRem()
print(constFeatRem, verbose = TRUE)
```



## Tuning CPO
CPOs export their parameters when attached to a learner. Tuning of CPO-Learners works exactly as tuning for ordinary Learners.

```{r}
(clrn = cpoModelMatrix() %>>% makeLearner("classif.logreg"))
getParamSet(clrn)

ps = makeParamSet(
    makeDiscreteParam(
        "model.matrix.formula",
        values = list(first = ~0 + ., second = ~0 + .^2, third = ~0 + .^3)))

tuneParams(clrn, pid.task, cv5, par.set = ps,
           control = makeTuneControlGrid(),
           show.info=TRUE)
```
Tuning of CPOs and tuning of Learners can happen at the same time. The following is not executed to save build time.
```{r, eval = FALSE}
tlrn = cpoModelMatrix() %>>%
       cpoWrap() %>>%
       cpoFilterGainRatio() %>>%
       makeLearner("classif.ctree")
sprintf("Parameters: %s", paste(names(getParamSet(tlrn)$pars), collapse=", "))
ps2 = makeParamSet(
    makeDiscreteParam(
        "model.matrix.formula",
        values = list(first = ~0 + ., second = ~0 + .^2)),
    makeDiscreteParam(
        "wrap.cpo",
        values = list(nopca = NULLCPO,
                      onlypca = cpoPca(),
                      addpca = cpoCbind(NULLCPO, cpoPca()))),
    makeDiscreteParam(
        "gain.ratio.perc",
        values = list(0.333, 0.667, 1.0)),
    makeDiscreteParam("teststat", values = c("quad", "max")))

tuneParams(tlrn, pid.task, cv5, par.set = ps2,
           control = makeTuneControlGrid())
```
